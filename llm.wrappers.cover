    1: from __future__ import annotations
       
    1: import asyncio
    1: import logging
    1: from abc import ABC, abstractmethod
    1: from dataclasses import dataclass
    1: from typing import AsyncIterator, List, Optional
       
    1: logger = logging.getLogger(__name__)
       
       
    2: class LLM(ABC):
    1:     """Abstract base interface for local LLM wrappers.
       
           The interface is fully offline and deterministic by default.
           """
       
    2:     @abstractmethod
    5:     async def generate_async(
               self,
    1:         prompt: str,
               *,
    2:         max_tokens: int = 16,
    1:     ) -> str:
               """Generate a text completion for a prompt as a single string."""
       
    2:     @abstractmethod
    5:     async def stream_async(
               self,
    1:         prompt: str,
               *,
    2:         max_tokens: int = 16,
    1:     ) -> AsyncIterator[str]:
               """Async generator yielding tokens/chunks for the completion."""
       
       
    3: @dataclass
    2: class LocalLLMConfig:
           # Performance knobs (deterministic, no randomness)
    1:     tokens_per_second: float = 120.0  # >80 t/s by default (target)
    1:     ttfb_ms: int = 50  # <500 ms TTFB by default (target)
    1:     max_concurrency: int = 5  # target concurrent requests
    1:     token_prefix: str = "token_"
           # If <=0, disable per-token delay for tests
    1:     disable_token_delay_if_leq_zero: bool = True
       
       
    2: class LocalLLM(LLM):
    1:     """A deterministic, offline local LLM wrapper.
       
           - Concurrency is limited via an asyncio.Semaphore.
           - Streaming yields synthetic tokens based on a simple counter.
           - Timing is simulated via asyncio.sleep; can be disabled in tests.
           """
       
    1:     def __init__(self, config: Optional[LocalLLMConfig] = None) -> None:
    3:         self.config = config or LocalLLMConfig()
    3:         if self.config.max_concurrency <= 0:
>>>>>>             raise ValueError("max_concurrency must be positive")
    3:         self._sem = asyncio.Semaphore(self.config.max_concurrency)
    3:         self._current_concurrency = 0
    3:         self._peak_concurrency = 0
    3:         self._cc_lock = asyncio.Lock()
       
    2:     @property
    2:     def peak_concurrency(self) -> int:
    1:         return self._peak_concurrency
       
    2:     @property
    2:     def current_concurrency(self) -> int:
>>>>>>         return self._current_concurrency
       
    1:     async def _inc_concurrency(self) -> None:
   24:         async with self._cc_lock:
   12:             self._current_concurrency += 1
   12:             if self._current_concurrency > self._peak_concurrency:
    7:                 self._peak_concurrency = self._current_concurrency
       
    1:     async def _dec_concurrency(self) -> None:
   24:         async with self._cc_lock:
   12:             self._current_concurrency -= 1
   12:             if self._current_concurrency < 0:
>>>>>>                 self._current_concurrency = 0
       
    1:     async def generate_async(self, prompt: str, *, max_tokens: int = 16) -> str:
   11:         tokens: List[str] = []
   46:         async for tok in self.stream_async(prompt, max_tokens=max_tokens):
   35:             tokens.append(tok)
   11:         return " ".join(tokens)
       
    1:     async def stream_async(self, prompt: str, *, max_tokens: int = 16) -> AsyncIterator[str]:
   12:         if not isinstance(prompt, str) or len(prompt) == 0:
>>>>>>             raise ValueError("prompt must be a non-empty string")
   12:         if max_tokens < 0:
>>>>>>             raise ValueError("max_tokens must be non-negative")
       
   12:         await self._sem.acquire()
   12:         await self._inc_concurrency()
   12:         try:
                   # Simulated TTFB
   12:             if self.config.ttfb_ms > 0:
   10:                 await asyncio.sleep(self.config.ttfb_ms / 1000.0)
       
                   # Generate deterministic tokens
   51:             for i in range(max_tokens):
   39:                 if not self._should_skip_token_delay():
   30:                     await self._sleep_per_token()
   39:                 yield f"{self.config.token_prefix}{i}"
               finally:
   12:             await self._dec_concurrency()
   12:             self._sem.release()
       
    1:     def _should_skip_token_delay(self) -> bool:
   39:         if self.config.disable_token_delay_if_leq_zero and self.config.tokens_per_second <= 0:
    9:             return True
   30:         return False
       
    1:     async def _sleep_per_token(self) -> None:
   30:         tps = self.config.tokens_per_second
   30:         if tps <= 0:
                   # No delay when disabled for tests
>>>>>>             return
   30:         await asyncio.sleep(1.0 / tps)
       
       
    1: __all__ = ["LLM", "LocalLLM", "LocalLLMConfig"]
